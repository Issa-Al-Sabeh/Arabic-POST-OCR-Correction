#!/usr/bin/env python3
"""
Test the fine-tuned AraFix model with sample OCR text
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the fine-tuned model
MODEL_PATH = "AraFix-Finetuned"

print("\n" + "="*80)
print("TESTING FINE-TUNED ARAFIX MODEL")
print("="*80)

print(f"\nğŸ“¦ Loading model from: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
print(f"âœ… Model loaded on: {device}")

# Test samples with OCR errors
test_samples = [
    # Sample 1: Missing spaces and character errors
    "Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠÂ» Ù†Ø¯ Ø¹Ùˆ Ø§Ù„Ù…Ø³Ø­ÙˆØ¨Ø©Ø¬Ù†Ø³ÙŠØ§ØªÙ‡Ù… Ø¥Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø­ØªÙ‡Ø§ Ù„ØªØ³Ù„Ù‘Ù… 'Ø¨Ø±Ø§Ø¡Ø©Ø°Ù…Ø©'",
    
    # Sample 2: Common OCR mistakes
    "Ø£Ø¹Ù„Ø«Øª ÙˆØ²Ø§Ø²Ø© Ø§Ù„ØªØ¹Ù„ÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠ Ø£Ù†Ù‡ ÙŠÙ†Ø¹ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙŠÙ† Ø³Ø­ÙŠØª Ù…ØªÙ‡Ù… Ø§Ù„Ø¬Ù†Ø³ÙŠØ©Ù…Ø±Ø§Ø¬Ø¹Ø©",
    
    # Sample 3: Merged words
    "Ù…ÙŠÙ†Ù‰ Ø§Ù„ÙˆØ²Ø§Ø±Ø© ÙÙŠ ØµØ§Ù„Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ† (Ø¨Ø±Ø¬ Ø§Ù„Ø³Ù†Ø§Ø¨Ù„ - 013), Ø§Ø¨ØªØ¯Ø§Ø¡ Ù…Ù† Ø§Ù„ÙŠÙˆÙ…Ø§Ù„Ø§Ø«Ù†ÙŠÙ†",
    
    # Sample 4: Number recognition errors
    "3 Ø§Ù„Ø¬Ø§Ø±ÙŠ Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© 10 ØµØ¨Ø§Ø­Ø§ Ø­Øª Ø§Ù„Ù…Ø§Ø¹Ø© 12 Ø¸Ù‡Ø±Ø§Ø¡ ÙˆØ°Ù„Ùƒ Ù„ØªØ³Ù„Ù… Ø´Ù‡Ø§Ø¯Ø© Ø¨Ø±Ø§Ø¡Ø©Ø°Ù…Ø©",
    
    # Sample 5: Mixed errors
    "ØµØ§Ø­Ø¨ Ø§Ù„Ø³Ù…Ù‚ Ø§Ù„Ø£Ù…ÙŠØ± Ø§Ù„Ø´Ø¨Ø¹ Ù…Ù…Ø¹Ù„ Ø§Ù…Ø¯ Ù…Ø³ØªÙ‚Ø¨Ù„Ø§ Ù†Ø§Ø¦Ø¨ Ø±Ø¦ÙŠØ³ Ù…Ø¬Ù„Ø³ Ø¥"
]

print(f"\n{'='*80}")
print("TESTING OCR CORRECTION")
print(f"{'='*80}\n")

for i, ocr_text in enumerate(test_samples, 1):
    print(f"{'â”€'*80}")
    print(f"Test {i}:")
    print(f"{'â”€'*80}")
    print(f"OCR Input:  {ocr_text}")
    
    # Prepare input (add prefix as used in training)
    input_text = f"correct: {ocr_text}"
    
    # Tokenize
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)
    
    # Generate correction
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            num_beams=5,  # Use beam search for better quality
            early_stopping=True
        )
    
    # Decode
    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Corrected:  {corrected_text}")
    print()

print(f"{'='*80}")
print("âœ… TESTING COMPLETE!")
print(f"{'='*80}\n")

# Interactive mode
print("ğŸ’¡ Want to test your own text? (Press Ctrl+C to exit)")
print()

try:
    while True:
        user_input = input("Enter OCR text to correct: ").strip()
        
        if not user_input:
            continue
        
        # Prepare and tokenize
        input_text = f"correct: {user_input}"
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(device)
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
        
        corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"â†’ Corrected: {corrected}\n")

except KeyboardInterrupt:
    print("\n\nğŸ‘‹ Goodbye!")
